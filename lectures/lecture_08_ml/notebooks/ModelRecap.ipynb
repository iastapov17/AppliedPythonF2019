{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML\n",
    "\n",
    "<b>Томас Митчел: “Компьютерная программа обучается на основе опыта E относительно некоторого класса задач T и меры качества P, если качество решения этих задач (относительно P) улучшается с приобретением опыта E”.</b>\n",
    "\n",
    "- Т - задача: Обучение с учителем, Обучение без учителя, Обучение с подкреплением, и тд.\n",
    "- P - метрика: Численное значение показывающее насколько хорошо алгоритм решает задачу.\n",
    "- E - опыт: Для алгоритмов это данные (с метками и без)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/ML.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Пайплайн машинного обучения</i>:\n",
    "- Задача\n",
    "- Метрика качества\n",
    "- Данные (признаковое описание объектов и целевые метки)\n",
    "- Правильная валидация (алгоритм должен быть обобщаемым)\n",
    "- Выбор модели (класса моделей)\n",
    "- Подготовка признаков (в соответствии с моделью)\n",
    "- Обучение (подбор параметров, оптимизируя метрику)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/Val.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Со временем так нельзя!!! Иначе заглянем в будущее</b>\n",
    "<img src=\"../imgs/TimeVal.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У модели есть параметры, которые оптимизируются в процессе обучения. Гиперпараметры задаются извне. Мы их подбираем на кросс-валидации. Например: веса в регрессии - это параметры, коэффициент регуляризации - гиперпараметр, который мы задаем извне. В деревьях, порог сплита - параметр, глубина дерева - гиперпараметр\n",
    "\n",
    "<b>Почему нельзя сделать коэффициент регуляризации параметром?</b>\n",
    "\n",
    "<img src=\"../imgs/Parameters.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/MLE.jpg\">\n",
    "<b>Maximum likelihood estimation</b>\n",
    "- Делаем предположение из какого распределения точки\n",
    "- Хотим максимизировать вероятность появления точек ВСЕХ одновременно -> подставим координаты в вероятность, перемножим\n",
    "- Максимизация произведения = максимизация логарифма произведения = максимизация суммы логарифмов\n",
    "- Максимизация $f(x)$ = минимизация $-f(x)$\n",
    "- На что можем влиять у $f(x)$ ? на параметры!\n",
    "- Получаем функцию, оптимизируемую относительно параметров - функцию потерь!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример для линейной регрессии:\n",
    "Для решения этой задачи нам нужно определиться с метрикой качества, которая покажет степень соответствия данной модели и тренировочных данных. Для этого воспользуемся методом максимизации правдоподобия.\n",
    "\n",
    "Правдоподобием называют вероятность того, что данная выборка была семплирована из данного распределения. Если объекты независимы и одинаково распределённы, то правдоподобие вычисляется как:\n",
    "$$\n",
    "    \\mathcal {L}_\\theta = \\prod_{i=1}^{N} P_\\theta(\\vec{x}_i)\n",
    "$$\n",
    "\n",
    "Т.к. большенство распределений параметрические, правдоподобие удобно использовать для того, чтобы оценить параметры распределения из которого появилась выборка. Для этого нужно найти такое $\\theta$ при котором правдоподобие будет максимальным. \n",
    "\n",
    "Посмотрим на нашу модель с вероятностной точки зрения.\n",
    "\n",
    "$$ \n",
    "y = \\mathbb{E}\\left[ p(t|x,w,\\sigma^{2}) \\right]\n",
    "$$\n",
    "\n",
    "Выпишем фунцию правдоподобия для нашего набора данных. Сразу возьмем логарифм правдоподобия, т.к. он поможет нам избавиться от произведения и степени экспоненты в нормальном распределении\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    " \\log \\left(\\mathcal {L}\\right) \n",
    "              &=& \\log \\prod_{i=1}^{N} \\mathcal{N}(\\vec{w}^T \\vec{x}_i,\\,\\sigma^{2}) \\\\\n",
    "              &=& \\sum_{i=1}^n \\log \\mathcal{N}\\left( \\vec{w}^T \\vec{x}_i, \\sigma^2 \\right) \\\\ \n",
    "              &=& \\sum_{i=1}^n \\log \\frac {1}{\\sigma {\\sqrt {2\\pi}}}\\;e^{-{\\frac {(y_i-\\vec{w}^T \\vec{x}_i )^{2}}{2\\sigma ^{2}}}} \\\\\n",
    "              &=& -n \\log \\sigma {\\sqrt {2\\pi}} -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \n",
    "                  \\left(y_i - \\vec{w}^T \\vec{x}_i\\right)^2\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Таким образом задача сводится к максимизации правдоподобия.\n",
    "$$ \n",
    "\\begin{array}{rcl}\n",
    "\\hat{w} &=& \\arg \\max_{w} log\\left(\\mathcal {L}\\right) \\\\ \n",
    "        &=& \\arg \\max_{w} -n \\log \\sigma {\\sqrt {2\\pi}} -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left(y_i - \\vec{w}^T \\vec{x}_i\\right)^2 \\\\ \n",
    "        &=& \\arg \\max_{w} - \\sum_{i=1}^n \\left(y_i - \\vec{w}^T \\vec{x}_i\\right)^2 \\\\ \n",
    "        &=& \\arg \\min_{w} L\\left(X, \\vec{y}, \\vec{w}\\right)\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как подобрать оптимальные параметры?\n",
    "### -> методы оптимизации\n",
    "<img src=\"../imgs/Optimization.png\">\n",
    "\n",
    "Один из простейших методов - градиентный спуск\n",
    "\n",
    "<img src=\"../imgs/GD.png\">\n",
    "\n",
    "Находим направление наискорейшего убывания функции - антиградиент\n",
    "\n",
    "<img src=\"../imgs/CostGradient.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача регрессии\n",
    "\n",
    "- $X$ - матрица-описание объектов, дизайн-матрица;\n",
    "- $Y$ - матрица-характеристики объектов;\n",
    "- $f$ - зависимость - некоторый метод подсчета характеристик объектов основываясь на их описании;\n",
    "- $\\left(X, Y = f\\left(X\\right)\\right)$ - наблюдения;\n",
    "- $S = \\left\\{X_i, Y_i\\right\\}_{i=1}^N$ - обучающая выборка - набор $N$ наблюдений - пары (описание объекта - значение);\n",
    "- $\\hat f$ - регрессионная модель - функция которая аппроксимирует исходную зависимость $f$.\n",
    "\n",
    "<img src=\"../imgs/Regression.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия\n",
    "\n",
    "$$\n",
    "y_i = w_0 + w_1 * x_{i1} + ... + w_m * x_{im} + \\epsilon \n",
    "$$\n",
    "\n",
    "Где $\\epsilon$ - шум, обычно предполагают $\\epsilon \\sim \\mathcal{N}(0,\\,\\sigma^{2})$ - с нулевым средним.\n",
    "\n",
    "- MSE (Mean Squared Error)\n",
    "\n",
    "$$\n",
    "MSE(\\hat{f}, x) = \\frac{1}{N} \\sum_{i=1}^{N}\\left(\\ y_i - \\hat{f}(x_i)\\ \\right) ^ 2\n",
    "$$\n",
    "\n",
    "\n",
    "- MAE (Mean Absolute Error)\n",
    "\n",
    "$$\n",
    "MAE(\\hat{f}, x) = \\frac{1}{N} \\sum_{i=1}^{N}\\left|\\ y_i - \\hat{f}(x_i)\\ \\right|\n",
    "$$\n",
    "\n",
    "\n",
    "- $R^2$ - коэффициент детерминации, показывает долю объясненной дисперсии (т.е. доля дисперсии объясненная моделью) в общей дисперсии целевой переменной.\n",
    "\n",
    "Данный коэффициент принимает значение от 0 до 1 (чем ближе к 1, тем лучше модель объясняет данные)\n",
    "\n",
    "(https://ru.wikipedia.org/wiki/Коэффициент_детерминации)\n",
    "\n",
    "$$\n",
    "R^2(\\hat{f}, x) = 1 - \\frac{\\sum_{i=1}^{N}\\left(\\ y_i - \\hat{f}(x_i)\\ \\right) ^ 2}{\\sum_{i=1}^{N}\\left(\\ y_i - \\bar{y}\\ \\right) ^ 2} \n",
    "$$, где\n",
    "$\\bar{y} = \\frac{1}{N}\\sum_{i=1}^{N} y_i$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Регуляризация </b> - это ограничения на наших параметры. Либо это предположение об их природе, но об этом когда-нибудь позже\n",
    "\n",
    "<b>L2</b>\n",
    "Можно просто добавить некоторый функционал который будет зависеть от весов регрессии\n",
    "\n",
    "$$\n",
    "MSE(\\hat{f}, x) = \\left|\\left| \\ y - X w\\ \\right|\\right|^2_2 + \\left|\\left| w \\right|\\right|^2_2 \\ \\to min\n",
    "$$\n",
    "\n",
    "https://ru.wikipedia.org/wiki/Норма_(математика)\n",
    "\n",
    "\n",
    "<b>L1</b>\n",
    "Выражение \n",
    "\n",
    "$$\n",
    "\\sum_{i=0}^{N}\\left(y_i - (w_2 * x_i ^ 2 + w_1 * x_i + w_0) \\right) ^ 2 + \\alpha \\sum_{j=1}^{2}\\left(w_i\\right)^2 \\to min\n",
    "$$\n",
    "\n",
    "\n",
    "Такая регрессия называется Lasso Regression\n",
    "\n",
    "$$\n",
    "MSE(\\hat{f}, x) = \\left|\\left| \\ y - X w\\ \\right|\\right|^2_2 + \\left|\\left| w \\right|\\right|_1 \\ \\to min\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=0}^{N}\\left(y_i - (w_2 * x_i ^ 2 + w_1 * x_i + w_0) \\right) ^ 2 + \\alpha \\sum_{j=1}^{2}\\left|w_i\\right| \\to min\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть X — множество описаний объектов, Y— множество номеров (или наименований) классов. Существует неизвестная целевая зависимость — отображение из X в Y, значения которой известны только на объектах конечной обучающей выборки.\n",
    "\n",
    "<img src=\"../imgs/ClassModel.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/LogReg.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функцию потерь получаем с помощью MLE и предположения о биномиальном распределении, то есть\n",
    "$$ p(x) = p*(1-p)$$\n",
    "\n",
    "<img src=\"../imgs/LogLoss.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же можем её регуляризовать, то есть наложить ограничения на параметры\n",
    "\n",
    "<img src=\"../imgs/LogRegReg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики классификации\n",
    "\n",
    "<img src=\"../imgs/PrRec.png\">\n",
    "\n",
    "Еще не забываем об f1:\n",
    "$$\n",
    "F_{\\beta} = (1 + \\beta^2) * \\frac{precision*recall}{(\\beta^2 * precision) + recall}\n",
    "$$\n",
    "\n",
    "Roc-AUC\n",
    "\n",
    "<img src=\"../imgs/RocAuc.png\">"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
